# E-commerce Analytics Pipeline - BÃ i Táº­p Thá»±c HÃ nh

ÄÃ¢y lÃ  má»™t bÃ i táº­p thá»±c hÃ nh hoÃ n chá»‰nh vá» **Data Engineering Pipeline** sá»­ dá»¥ng **Python**, **Apache Airflow**, **Grafana**, vÃ  **PostgreSQL**.

## ğŸ¯ Má»¥c TiÃªu Há»c Táº­p

Thá»±c hÃ nh xÃ¢y dá»±ng má»™t **End-to-End Data Pipeline** vá»›i:

- **ETL Processing** vá»›i Apache Airflow
- **Data Visualization** vá»›i Grafana
- **Data Warehouse** vá»›i PostgreSQL
- **Monitoring & Alerting**

## ğŸ—ï¸ Kiáº¿n TrÃºc Há»‡ Thá»‘ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Source Data â”‚â”€â”€â”€â–¶â”‚   Airflow   â”‚â”€â”€â”€â–¶â”‚Data Warehouseâ”‚â”€â”€â”€â–¶â”‚   Grafana   â”‚
â”‚ (PostgreSQL)â”‚    â”‚    ETL      â”‚    â”‚ (PostgreSQL) â”‚    â”‚ Dashboards  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ CÃ¡c ThÃ nh Pháº§n

### 1. **Apache Airflow**

- **Vai trÃ²**: Orchestration platform - quáº£n lÃ½ workflow ETL
- **Chá»©c nÄƒng**:
  - Láº­p lá»‹ch cháº¡y ETL jobs hÃ ng ngÃ y
  - Extract tá»« source tables
  - Transform data (customer segmentation, product categorization)
  - Load vÃ o data warehouse
  - Data quality checks

### 2. **PostgreSQL**

- **Source Schema (`source_data`)**: Giáº£ láº­p production database
  - `customers`, `products`, `orders`, `order_items`
- **Target Schema (`target_data`)**: Data warehouse
  - `dim_customers`, `dim_products`, `fact_sales`, `daily_sales_summary`

### 3. **Grafana**

- **Vai trÃ²**: Data visualization vÃ  monitoring
- **Dashboards**:
  - KPI metrics (Revenue, Orders, Customers)
  - Time series charts
  - Category analysis
  - Customer insights

## ğŸš€ CÃ¡ch Cháº¡y Project

### BÆ°á»›c 1: Clone vÃ  Setup

```bash
# Clone project (náº¿u tá»« git)
git clone <repo-url>
cd Lotus

# Táº¡o environment file
echo 'AIRFLOW_UID=50000' > .env
echo 'AIRFLOW_GID=0' >> .env
```

### BÆ°á»›c 2: Khá»Ÿi Ä‘á»™ng Docker Services

```bash
# Khá»Ÿi Ä‘á»™ng táº¥t cáº£ services
docker-compose up -d

# Kiá»ƒm tra status
docker-compose ps
```

### BÆ°á»›c 3: Setup Airflow Connection

1. Truy cáº­p Airflow UI: `http://localhost:8080`
2. Login: `admin` / `admin`
3. Táº¡o Connection:
   - **Conn Id**: `postgres_data`
   - **Conn Type**: `Postgres`
   - **Host**: `postgres_data`
   - **Schema**: `ecommerce`
   - **Login**: `datauser`
   - **Password**: `datapass`
   - **Port**: `5432`

### BÆ°á»›c 4: Cháº¡y ETL Pipeline

1. Trong Airflow UI, tÃ¬m DAG `ecommerce_etl_pipeline`
2. Enable DAG (toggle button)
3. Trigger DAG manually Ä‘á»ƒ test
4. Monitor task execution

### BÆ°á»›c 5: Truy cáº­p Grafana Dashboard

1. Truy cáº­p: `http://localhost:3000`
2. Login: `admin` / `admin`
3. Dashboard sáº½ tá»± Ä‘á»™ng load tá»« provisioning

## ğŸ“Š Sample Data

Project bao gá»“m sample data cho:

- **5 customers** tá»« cÃ¡c thÃ nh phá»‘ khÃ¡c nhau
- **8 products** trong 4 categories (Electronics, Furniture, Sports, Kitchenware)
- **5 orders** vá»›i cÃ¡c tráº¡ng thÃ¡i khÃ¡c nhau
- **8 order items** chi tiáº¿t

## ğŸ” CÃ¡c Thao TÃ¡c Thá»±c HÃ nh

### 1. ETL Development

```python
# Customize transformation logic trong DAG
# File: airflow/dags/ecommerce_etl_dag.py

# VÃ­ dá»¥: ThÃªm customer segmentation rule má»›i
def extract_and_transform_customers():
    # Modify customer segmentation logic
    extract_query = """
    SELECT customer_id,
           CASE 
               WHEN total_spent >= 1000 THEN 'VIP'
               WHEN total_spent >= 500 THEN 'Premium'
               ELSE 'Regular'
           END as customer_segment
    FROM ...
    """
```

### 2. Data Quality Checks

```sql
-- ThÃªm data quality rules
SELECT 
    COUNT(*) as total_customers,
    COUNT(CASE WHEN email IS NULL THEN 1 END) as missing_emails,
    COUNT(CASE WHEN customer_segment IS NULL THEN 1 END) as missing_segments
FROM target_data.dim_customers;
```

### 3. Dashboard Customization

- ThÃªm panels má»›i trong Grafana
- Táº¡o alerts cho metrics quan trá»ng
- Customize visualization types

### 4. Database Operations

```bash
# Connect vÃ o PostgreSQL
docker exec -it <postgres_container> psql -U datauser -d ecommerce

# Explore data
\dt source_data.*
\dt target_data.*

# Query examples
SELECT * FROM target_data.daily_sales_summary;
```

## ğŸ¯ BÃ i Táº­p Má»Ÿ Rá»™ng

### Level 1: Beginner

1. **ThÃªm sáº£n pháº©m má»›i** vÃ o source database
2. **Cháº¡y láº¡i ETL** vÃ  quan sÃ¡t káº¿t quáº£
3. **Táº¡o dashboard panel má»›i** cho top selling products

### Level 2: Intermediate

1. **ThÃªm DAG má»›i** Ä‘á»ƒ import data tá»« CSV file
2. **Táº¡o SLA alerts** trong Airflow cho tasks cháº¡y quÃ¡ lÃ¢u
3. **Implement incremental loading** thay vÃ¬ full refresh

### Level 3: Advanced

1. **ThÃªm multiple data sources** (MySQL, Oracle simulation)
2. **Implement change data capture** (CDC) pattern
3. **Create automated testing** cho ETL logic
4. **Setup monitoring alerts** trong Grafana

## ğŸ”§ Troubleshooting

### Common Issues:

1. **Airflow tasks fail**:

   ```bash
   # Check logs
   docker logs <airflow_container>

   # Reset DB
   docker-compose down -v
   docker-compose up -d
   ```
2. **Database connection issues**:

   ```bash
   # Test connection
   docker exec -it <postgres_container> pg_isready -U datauser
   ```
3. **Grafana dashboard khÃ´ng hiá»ƒn thá»‹ data**:

   - Kiá»ƒm tra PostgreSQL connection trong Grafana
   - Verify data cÃ³ trong target tables
   - Check dashboard time range

## ğŸ“š Learning Resources

### Airflow Concepts:

- **DAG**: Directed Acyclic Graph - workflow definition
- **Task**: Individual unit of work
- **Operator**: Defines what task does (PythonOperator, SQLOperator)
- **Hook**: Interface to external systems (PostgresHook)

### ETL Best Practices:

- **Idempotency**: ETL should produce same result when re-run
- **Data Quality**: Always validate data after transformation
- **Monitoring**: Track success/failure rates vÃ  execution time
- **Documentation**: Comment code vÃ  maintain data lineage

### Performance Tips:

- **Indexing**: Create indexes on frequently queried columns
- **Partitioning**: Partition large tables by date
- **Parallel Processing**: Run independent tasks in parallel
- **Connection Pooling**: Reuse database connections

## ğŸŒŸ Next Steps

Sau khi hoÃ n thÃ nh bÃ i táº­p nÃ y, báº¡n cÃ³ thá»ƒ:

1. **Há»c vá» real-time streaming** vá»›i Apache Kafka
2. **Implement cloud deployment** vá»›i AWS/GCP/Azure
3. **Explore advanced analytics** vá»›i dbt (data build tool)
4. **Study MLOps pipeline** integration

## ğŸ“ Support

Náº¿u gáº·p váº¥n Ä‘á» trong quÃ¡ trÃ¬nh thá»±c hÃ nh, hÃ£y:

1. Check logs trong Docker containers
2. Verify database connections
3. Review Airflow task logs
4. Test individual components

---

## How to run gerage data
```py
cd ~/Desktop/Lotus
python3 -m venv venv
source venv/bin/activate
pip install psycopg2-binary faker
python scripts/generate_sample_data.py
```


**Happy Learning! ğŸš€**

*BÃ i táº­p nÃ y giÃºp báº¡n hiá»ƒu Ä‘Æ°á»£c end-to-end data pipeline trong thá»±c táº¿ vÃ  cÃ¡ch cÃ¡c cÃ´ng cá»¥ lÃ m viá»‡c cÃ¹ng nhau.*
